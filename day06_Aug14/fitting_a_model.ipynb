{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run first: importing our modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fitting a line to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some noisy data that loosely follows a line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some noisy data\n",
    "# first, create the time steps along the x axis: every 0.1 units for 4 units\n",
    "x = np.arange(0,4,step=0.1)\n",
    "\n",
    "# generate some noise with the same size as 'x'\n",
    "noise = np.random.normal(size=np.shape(x))\n",
    "\n",
    "# create a 'true' model with parameters a_true and b_true, somewhere between 0 and 5.\n",
    "# These are the 'hidden' values we want to know.\n",
    "# Note: they will be different each time you run this cell, be careful!\n",
    "a_true = 5*np.random.rand()\n",
    "b_true = 5*np.random.rand()\n",
    "\n",
    "# create the y-values and add some noise.\n",
    "# Question: how does the data look if you take out the noise?\n",
    "y = a_true * x + b_true + noise\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x,y,'k.',label='noisy data')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment: Try to fit the data by hand by guessing the values of a and b until the model looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "b=5\n",
    "model = a*x + b\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x,y,'k.',label='noisy data')\n",
    "plt.plot(x,model,'-r',label='my model')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I estimated \", a, \" for a, the true value was \", a_true)\n",
    "print(\"I estimated \", b, \" for b, the true value was \", b_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Interactive plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a random seed to change the random result that we get each time we run the code.\n",
    "# we have to put this outside the function \n",
    "np.random.seed()\n",
    "rseed = np.random.randint(100)\n",
    "print(rseed)\n",
    "\n",
    "def plot_interactive_line(numdata=20, a=2., b=2., noiselevel=2.):\n",
    "\n",
    "    # User-defined model:\n",
    "    xmax=10\n",
    "    x = np.linspace(-xmax, xmax, int(100*xmax))\n",
    "    y = a*x + b\n",
    " \n",
    "    # create random noisy data\n",
    "    # get a random a between (0,5) and b between (0,5)\n",
    "    np.random.seed(rseed)\n",
    "    a_true = -5 + 10 * np.random.rand()\n",
    "    b_true = -5 + 10 * np.random.rand()\n",
    "    \n",
    "    # create some synthetic GPS locations\n",
    "    xdata = np.linspace(-0.9*xmax,0.9*xmax,numdata) + np.random.normal(loc=0,scale=1,size=numdata)\n",
    "    \n",
    "    # create the synthetic model\n",
    "    ymodel = a_true*xdata + b_true\n",
    "    \n",
    "    #add gaussian noise: mean of 0, standard deviation of 'noiselevel'\n",
    "    ynoise = np.random.normal(loc=0,scale=noiselevel,size=numdata)\n",
    "    \n",
    "    #get the final 'observed' data\n",
    "    ydata = ymodel + ynoise\n",
    "    \n",
    "    # compute the model from the input parameters\n",
    "    ydata_predicted = a + b*xdata\n",
    "    \n",
    "    # compute the total misfit ('chi-squared' statistic): \n",
    "    # this is the sum of the misfits, divided by number of data and the data noise\n",
    "    chi_squared = np.sum( (ydata - ydata_predicted)**2 )/(numdata*noiselevel**2)\n",
    "    \n",
    "    # prepare the plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    \n",
    "    # prepare the axes limits\n",
    "    ax.set_xlim((-xmax,xmax))\n",
    "    ax.set_ylim((-50,50))\n",
    "    \n",
    "    #ax.plot(x, y, 'bx')\n",
    "    points = ax.errorbar(xdata, ydata, fmt='ko', yerr=noiselevel, label='Data',capsize=2)\n",
    "    lineref = ax.plot(x, y, 'b-',label = \"Model: a=%.1f, b=%.1f\"%(a,b))\n",
    "    ax.text(-0.9*xmax,-45,'misfit = %.2f'%chi_squared)\n",
    "    \n",
    "    plt.setp(lineref, linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return a_true, b_true, xdata, ydata\n",
    "\n",
    "\n",
    "# now, we create the interactive plot with the special jupyter commands interactive() and display(). \n",
    "w = interactive(plot_interactive_line, numdata=(1,100), a=(-5.,5.), b=(-5.,5.),noiselevel=(0.5,5.))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close did you get this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The true value of a was \", w.result[0])\n",
    "print(\"The true value of b was \", w.result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the interactive code block above does not work:\n",
    "\n",
    "Open up the anaconda prompt (search for anaconda prompt in the anaconda window), and run the following commands:\n",
    "\n",
    "    conda install nodejs\n",
    "    conda install -c conda-forge ipywidgets\n",
    "    jupyter labextension install @jupyter-widgets/jupyterlab-manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting the data with python automatically\n",
    "\n",
    "Since least-squares fitting is such a common activity, there are nice, easy functions in python to do it. Let's use one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.polyfit works similar to matlab polyfit(). the \"order\" of fitting (1 in this case) is the highest power of x.\n",
    "# Fitted coefficients are returned in order of highest power to lowest.\n",
    "m_fit = np.polyfit(x, y, 1)\n",
    "\n",
    "# compute the predicted y values:\n",
    "# np.polyval() does the same thing as this line:\n",
    "# y_predicted = m_fit[0]*x + m_fit[1]\n",
    "\n",
    "y_predicted = np.polyval(m_fit, x)\n",
    "\n",
    "plt.plot(x,y,'k.',label='noisy data')\n",
    "plt.plot(x,y_predicted, '-r', label='least-squares fit')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "print('Numpy estimated values are: a =',m_fit[0],'     b =',m_fit[1])\n",
    "print('The true values are:   a_true =',a_true,'b_true =',b_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More advamced fitting: non-polynomial functions\n",
    "\n",
    "The above method is great, as long as we want to fit a linear function to our data (or a quadratic, or a cubic, etc.).\n",
    "\n",
    "What if we want to fit something that's not a line? In this case, we need something more advanced. Here we can use `scipy.optimize.curve_fit` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this method, we define our linear model as a function. \n",
    "# The first argument is the x data, and the rest of the arguments are the parameters to fit.\n",
    "\n",
    "def my_line(x,a,b):\n",
    "    return a*x + b\n",
    "\n",
    "# here, the inputs are the function name, the x data, and the y data. \n",
    "# finally p0 is the 'initial guess' for this method. \n",
    "# The outputs are m and m_cov.\n",
    "# m is the model output, and mcov is the model covariance, or uncertainties.\n",
    "\n",
    "m,mcov = scipy.optimize.curve_fit(my_line,x,y,p0=[0,0])\n",
    "\n",
    "print(m)\n",
    "\n",
    "# to get the predicted y values, we can just run our function with the values of m.\n",
    "y_predicted=my_line(x,m[0],m[1]) \n",
    "\n",
    "plt.plot(x,y,'k.',label='noisy data')\n",
    "plt.plot(x,y_predicted, '-r', label='least-squares fit')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus. Fitting a model with linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the model function y=a*x+b, we can express this in matrix notation as follows:\n",
    "\n",
    "    [y1] = [x1  1]  *  [a]\n",
    "    [y2]   [x2  1]     [b]\n",
    "    [y3]   [x3  1]\n",
    "    [...]  [ ... ]  \n",
    "\n",
    "The matrix on the left is an Nx1 column of the y-values, and on the right we have an Nx2 matrix filled with first a column of the x-values, and then a column of ones, multiplied into the 2x1 matrix with the parameters we want to fit - a and b. The result of multiplying a [Nx2] with [2x1] matrix is [Nx1], so we can see that the matrix dimensions work. \n",
    "\n",
    "We can actually express *any* model that is a linear function of the parameters in this way; in Geophysics this is typically written as solving the problem $Gm=d$, where $G$ is our \"design matrix\" (in this case, filled with ones and x-values), $d$ is our \"data\" matrix of the measured y-values, and $m$ is the model parameters we want to estimate; in this case, the y-intercept $a$ and slope of the line $b$.\n",
    "\n",
    "Using the theorems of linear algebra, we can show that the solution to the matrix problem $Gm=d$ can be written as $m=(G^TG)^{-1}G^Td$. This equation is scary to look at, but in the computer we can work it out pretty quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. First we create our 'design matrix'. For a linear model, this is a column of ones, next to a column of the x-coordinate of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = np.ones(np.shape(x))\n",
    "col2 = x\n",
    "# use np.column_stack to line up arrays as columns in a matrix. There is a similar function for rows, np.row_stack.\n",
    "G = np.column_stack( (col1,col2) )\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment: compute the best-fitting line! Recall our equation: \n",
    "\n",
    "$m=(G^TG)^{-1}G^Td$\n",
    "\n",
    "Use the numpy commands for transpose and inverse:\n",
    "\n",
    "    np.transpose()\n",
    "    np.linalg.inv()\n",
    "    \n",
    "And, to multiply matrices, use the .dot() method of numpy arrays, like so:\n",
    "\n",
    "    AtimesB = A.dot(B)\n",
    "\n",
    "Look out for typos, it's a lot to type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the value of m = (G^T * G)^-1 * G^T * y\n",
    "# step 1: define a new variable GT as the transpose of G\n",
    "\n",
    "\n",
    "# step 2: define GTG as GT * G\n",
    "\n",
    "\n",
    "# step 3: define GTGinv as GTG^-1\n",
    "\n",
    "\n",
    "# step 4: GTGInvGT = GTGInv * GT\n",
    "\n",
    "\n",
    "#step 5: get the final result, m, as GTGinvGT * y.\n",
    "\n",
    "\n",
    "# print out the result, and compare to the true values from above.\n",
    "print(m)\n",
    "print(a_true,b_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, these values should be pretty close to our true values but not exactly - this is because we added noise! In general, the presence of noise means we will never be able to exactly measure the **true model** values. \n",
    "\n",
    "Now, let's plot the model on top of the data and see how it fits. Remember, we can get our \"predicted\" data (y-values of the model) with the equation $d = G * m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted = \n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x,y,'k.',label='noisy data')\n",
    "plt.plot(x,data_predicted, '-r', label='least-squares fit')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
